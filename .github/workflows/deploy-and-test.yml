name: Deploy and Test

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      video:
        description: 'Video to test'
        required: true
        default: 'adam'
        type: choice
        options:
          - adam
          - angela
          - audrey

env:
  AWS_REGION: us-east-2
  ECR_REPOSITORY: multimodal-transcription
  S3_BUCKET: multimodal-transcription-videos-1761690600

jobs:
  deploy-and-test:
    name: Deploy and Test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Set video path
      id: video-path
      run: |
        case "${{ github.event.inputs.video || 'adam' }}" in
          "adam")
            echo "video-path=test-videos/Adam_2024-03-03_6_32_PM.mp4" >> $GITHUB_OUTPUT
            echo "video-name=Adam" >> $GITHUB_OUTPUT
            ;;
          "angela")
            echo "video-path=test-videos/Angela_2025-03-10_2_11_PM.mp4" >> $GITHUB_OUTPUT
            echo "video-name=Angela" >> $GITHUB_OUTPUT
            ;;
          "audrey")
            echo "video-path=test-videos/Audrey_2025-04-06_6_20_PM-2.mp4" >> $GITHUB_OUTPUT
            echo "video-name=Audrey" >> $GITHUB_OUTPUT
            ;;
        esac

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Build and push Docker image
      id: build-image
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        echo "Building Docker image..."
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
        echo "Pushing to ECR..."
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        echo "image-uri=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

    - name: Test with video
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        OUTPUT_PREFIX="test-outputs/${{ steps.video-path.outputs.video-name }}-$TIMESTAMP"
        LOCAL_OUTPUT_DIR="/tmp/transcription-output"
        
        echo "üé¨ Testing video: ${{ steps.video-path.outputs.video-name }}"
        echo "üìÅ Local output: $LOCAL_OUTPUT_DIR"
        echo "üìÅ S3 output: s3://${{ env.S3_BUCKET }}/$OUTPUT_PREFIX/"
        
        # Create local directories with proper permissions
        mkdir -p $LOCAL_OUTPUT_DIR
        chmod 777 $LOCAL_OUTPUT_DIR
        
        # Download video from S3 to local
        LOCAL_VIDEO_PATH="/tmp/input-video.mp4"
        echo "üì• Downloading video from S3..."
        aws s3 cp "s3://${{ env.S3_BUCKET }}/${{ steps.video-path.outputs.video-path }}" $LOCAL_VIDEO_PATH
        
        # Test the ECR image with local video and output directory
        docker run --rm \
          -v $LOCAL_VIDEO_PATH:/app/input-video.mp4:ro \
          -v $LOCAL_OUTPUT_DIR:/app/outputs \
          -e GOOGLE_API_KEY="${{ secrets.GOOGLE_API_KEY }}" \
          -e AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}" \
          -e AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
          -e AWS_DEFAULT_REGION="${{ env.AWS_REGION }}" \
          ${{ steps.build-image.outputs.image-uri }} \
          python src/transcription_pipeline.py \
          --input "/app/input-video.mp4" \
          --output-dir "/app/outputs" \
          --chunk-size 600 \
          --max-workers 2
        
        # Upload results to S3 (exclude videos/ and chunks/ directories)
        echo "üì§ Uploading results to S3..."
        aws s3 sync $LOCAL_OUTPUT_DIR/ "s3://${{ env.S3_BUCKET }}/$OUTPUT_PREFIX/" \
          --exclude "*/videos/*" \
          --exclude "*/chunks/*" \
          --exclude "*/raw_responses/*"
        
        # Verify upload was successful
        echo "‚úÖ Verifying S3 upload..."
        aws s3 ls "s3://${{ env.S3_BUCKET }}/$OUTPUT_PREFIX/" --recursive | head -10
        
        # Clean up local files with proper permissions
        echo "üßπ Cleaning up local files..."
        
        # Change ownership of files created by Docker container
        sudo chown -R runner:runner $LOCAL_OUTPUT_DIR 2>/dev/null || true
        
        # Remove files with proper permissions
        sudo rm -rf $LOCAL_OUTPUT_DIR
        rm -f $LOCAL_VIDEO_PATH
        
        echo "‚úÖ Local cleanup completed successfully"
        
        # Store output prefix for results step
        echo "OUTPUT_PREFIX=$OUTPUT_PREFIX" >> $GITHUB_ENV

    - name: Clean up old ECR images
      run: |
        echo "üßπ Cleaning up old ECR images..."
        
        # Get current image digest using the image URI
        CURRENT_IMAGE_DIGEST=$(aws ecr describe-images --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} --query 'imageDetails[0].imageDigest' --output text)
        
        if [ "$CURRENT_IMAGE_DIGEST" != "None" ] && [ ! -z "$CURRENT_IMAGE_DIGEST" ]; then
          echo "Current image digest: $CURRENT_IMAGE_DIGEST"
          
          # List all images except the current one
          ALL_IMAGES=$(aws ecr list-images --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} --query 'imageIds[?imageDigest!=`'$CURRENT_IMAGE_DIGEST'`].imageDigest' --output text)
          
          if [ ! -z "$ALL_IMAGES" ]; then
            echo "Deleting old images..."
            for digest in $ALL_IMAGES; do
              echo "Deleting: $digest"
              aws ecr batch-delete-image --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} --image-ids imageDigest=$digest || echo "Failed to delete $digest (may be referenced by manifest list)"
            done
          else
            echo "No old images to clean up"
          fi
        else
          echo "No current image found, skipping cleanup"
        fi
        
        echo "‚úÖ ECR cleanup completed"

    - name: Clean up unused CloudWatch log groups
      run: |
        echo "üßπ Cleaning up unused CloudWatch log groups..."
        
        # List of log groups to clean up (keep only the one we're using)
        LOG_GROUPS_TO_DELETE=(
          "/ecs/multimodal-transcription-simple"
          "/ecs/multimodal-transcription-test"
        )
        
        for log_group in "${LOG_GROUPS_TO_DELETE[@]}"; do
          echo "Checking log group: $log_group"
          if aws logs describe-log-groups --log-group-name-prefix "$log_group" --region ${{ env.AWS_REGION }} --query 'logGroups[0].logGroupName' --output text | grep -q "$log_group"; then
            echo "Deleting log group: $log_group"
            aws logs delete-log-group --log-group-name "$log_group" --region ${{ env.AWS_REGION }} || echo "Failed to delete $log_group (may have active streams)"
          else
            echo "Log group $log_group not found or already deleted"
          fi
        done
        
        echo "‚úÖ CloudWatch cleanup completed"

    - name: Show results
      run: |
        echo "‚úÖ Test completed successfully!"
        echo "üé¨ Video: ${{ steps.video-path.outputs.video-name }}"
        echo "üì¶ Image: ${{ steps.build-image.outputs.image-uri }}"
        echo "üìÅ Results: s3://${{ env.S3_BUCKET }}/${{ env.OUTPUT_PREFIX }}/"
        echo ""
        echo "üìã S3 Output files:"
        aws s3 ls "s3://${{ env.S3_BUCKET }}/${{ env.OUTPUT_PREFIX }}/" --recursive || echo "No files found"
